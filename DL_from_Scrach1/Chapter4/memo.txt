4章　ニューラルネットワークの学習
【主題】
・ニューラルネットワークの最適な重みを訓練データから獲得すること
・本章では損失関数を導入し、関数の出力がより小さくなるように重みを探し出す
・損失関数のできるだけ小さな数値(最小ではない)を発見するための勾配法の説明

4.1データから学習する
4.1.1データ駆動
ニューラルネットワークのパラメータ(重み)は数千、万、億といった膨大な数になるので
「人力」でなく「機械」によって設定するために「データ」を用いる
データから的確な出力を得るために「特徴量」を抽出する
特徴量・・・入力データから本質的なデータを的確に抽出できるように設計された変換機
　　　　　　（コンピュータビジョンではSIFT,SURF,HOGなど、機械学習ではSVM,KNNなど）
ディープラーニングではニューラルネットワークが特徴量を抽出する

4.1.2訓練データとテストデータ
　学習を行う際にはデータを「訓練データ（教師データ）」と「テストデータ」に分ける
　訓練データを使って学習を行い、テストデータにて汎化能力（パラメータの適合度）を評価する
　また、データセット（特定の訓練データ）に過度に対応してしまうことを過学習(overfitting)という
　過学習は汎化能力が低いため、機械学習では避けるべき課題である

4.2損失関数
ニューラルネットワークの最適パラメータを探索学習では「損失関数」を利用する

4.2.1二乗和誤差(mean squared error)の関数
ニューラルネットワークの出力と正解となる教師データの差が
関数に影響を及ぼす
正解のラベルが「1」不正解のラベルが「0」であるため、
出力が正解ラベルと適合しているほど数値が0に近づく

#交差エントロピー誤差
正解ラベルに対応する出力の自然対数を計算する
正解ラベルの出力が0の場合にlog0がマイナス無限大になってしまうので
実装の際には出力に微小な数値を足すことでエラーを回避する

4.2.3ミニバッチ学習
　4.2.1および4.2.2では単一の訓練データに対する対する
損失関数を求めたが、実際にはすべての訓練データを対象
としてとするため、全訓練データの損失関数の和を指標とする。
また、得た損失関数の和はデータ数で割り、正規化を行う
　ただし、学習データが膨大な場合にすべてのデータを対象とするのは
計算量的に現実的でないため、データの一部(ミニバッチ)を選び、
そのミニバッチごとに学習を行うことで学習を行う。
この学習方法を「ミニバッチ学習」という

4.2.5なぜ損失関数を設定するのか？
　ニューラルネットワークの学習ではパラメータの微分（勾配）を計算することで、
損失関数の数値が小さくなるパラメータの数値を探し、パラメータの更新を行う。
　ニューラルネットワークのパラメータの一つに着目して考えると、
「損失関数に対するそのパラメータの微分はパラメータを変化させた際に
　損失関数がどのように変化するかをを示す」
つまり微分の値がマイナスであればパラメータを大きくすることで損失関数を
小さくすることができる（逆もしかり）。
また、微分の数値が0であればそれ以上損失関数は変化しない

↑活性化関数をステップ関数に設定すると微分の数値がほとんどの場所で0になってしまい、学習が行うことができない

・認識精度を指標にしない理由
　認識精度を指標にした場合、パラメータの小さな調整では制度の変化が起こりずらいが(ステップ関数的な不連続変化)、
　損失関数を指標にした場合、パラメータの調整によって損失関数も連続的に変化する(交差エントロピー誤差はわかりやすく非線形)

4.3数値微分
　勾配法・・・勾配(パラメータの微分の値)を使いパラメータを最適な値に近似していく
4.3.1微分
　微分・・・瞬間的な変化量(変化割合)のこと
4.3.3数値微分
　